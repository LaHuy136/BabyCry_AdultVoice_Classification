{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalized Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_normalized_energy(frames):\n",
    "    \"\"\"Tính năng lượng chuẩn hóa của từng khung.\"\"\"\n",
    "    energy = np.sum(frames**2, axis=1)\n",
    "    if np.max(energy) > 0:\n",
    "        normalized_energy = energy / np.max(energy)  # Chuẩn hóa nếu max > 0\n",
    "    else:\n",
    "        normalized_energy = energy  # Giữ nguyên nếu max = 0\n",
    "\n",
    "    return normalized_energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caculated F0 Using HPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hps_f0(signal, sample_rate, min_freq=60, max_freq=800, N_FFT=2048):\n",
    "    # Áp dụng cửa sổ Hamming để làm mượt tín hiệu\n",
    "    windowed_signal = signal * np.hamming(len(signal))\n",
    "    \n",
    "    # Thực hiện FFT và chỉ lấy phổ biên độ\n",
    "    fft_N_points = np.fft.fft(windowed_signal, N_FFT)\n",
    "    spectrum = 2.0/N_FFT * np.abs(fft_N_points[:N_FFT//2])\n",
    "    frequencies_N_points = sample_rate * np.arange(N_FFT//2) / N_FFT\n",
    "    \n",
    "    # Giới hạn tần số trong khoảng quan tâm (80Hz - 400Hz)\n",
    "    valid_freqs = (frequencies_N_points >= min_freq) & (frequencies_N_points <= max_freq)\n",
    "    frequencies_N_points = frequencies_N_points[valid_freqs]\n",
    "    spectrum = spectrum[valid_freqs]\n",
    "    # Áp dụng HPS (Harmonic Product Spectrum)\n",
    "    hps_spectrum = np.copy(spectrum)\n",
    "\n",
    "    # Nhân phổ với các bội số 2, 3, 4,...\n",
    "    for h in range(2, 4):\n",
    "        # Downsample bằng cách sử dụng phép nội suy để lấy phổ tương ứng\n",
    "        downsampled_spectrum = np.interp(\n",
    "            np.arange(0, len(spectrum), h),  # Các giá trị sau khi nội suy\n",
    "            np.arange(0, len(spectrum)),     # Các giá trị ban đầu\n",
    "            spectrum                        # Phổ gốc\n",
    "        )\n",
    "        \n",
    "        # Đảm bảo các phổ có cùng độ dài trước khi nhân\n",
    "        min_len = min(len(hps_spectrum), len(downsampled_spectrum))\n",
    "        hps_spectrum[:min_len] *= downsampled_spectrum[:min_len]\n",
    "        log_spectrum = np.log(np.abs(spectrum) + np.finfo(float).eps)  # Thêm epsilon để tránh log(0)\n",
    "        \n",
    "    # Tìm tần số có biên độ lớn nhất sau khi áp dụng HPS\n",
    "    peak_index = np.argmax(log_spectrum)\n",
    "    peak_freq = frequencies_N_points[peak_index]\n",
    "    return peak_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detect Baby Cry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_audio_class(file_path, frame_length_ms=30, frame_step_ms=15, sr=16000, \n",
    "                       energy_threshold=0.001, f0_threshold=400):\n",
    "\n",
    "    signal, sr = librosa.load(file_path, sr=sr)\n",
    "\n",
    "    frame_length = int(frame_length_ms * sr / 1000)\n",
    "    frame_step = int(frame_step_ms * sr / 1000)\n",
    "    frames = librosa.util.frame(signal, frame_length=frame_length, hop_length=frame_step).T\n",
    "    \n",
    "    normalized_energy = compute_normalized_energy(frames)\n",
    "\n",
    "    silence_count = 0\n",
    "    voice_count = 0\n",
    "    cry_count = 0\n",
    "\n",
    "    for frame, energy in zip(frames, normalized_energy):\n",
    "        if energy < energy_threshold:\n",
    "            silence_count += 1\n",
    "        else:\n",
    "            f0 = hps_f0(frame, sr) \n",
    "            if f0 > f0_threshold:\n",
    "                cry_count += 1\n",
    "            else:\n",
    "                voice_count += 1\n",
    "\n",
    "    # In thông tin phân loại\n",
    "    print(f\"Frames classified as Silence: {silence_count}\")\n",
    "    print(f\"Frames classified as Voice: {voice_count}\")\n",
    "    print(f\"Frames classified as Cry: {cry_count}\")\n",
    "\n",
    "    # Quyết định lớp dựa trên số lượng khung\n",
    "    if silence_count > max(voice_count, cry_count):\n",
    "        return \"Silence\"\n",
    "    elif cry_count > voice_count:\n",
    "        return \"Baby Cry\"\n",
    "    else:\n",
    "        return \"Adult Voice\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames classified as Silence: 0\n",
      "Frames classified as Voice: 656\n",
      "Frames classified as Cry: 8\n",
      "Audio is classified as: Adult Voice\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file_path = \"pbl6_01_audio.wav\"\n",
    "    \n",
    "    classification = detect_audio_class(file_path)\n",
    "    print(f\"Audio is classified as: {classification}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_audio_classification(dataset_path, frame_length_ms=30, frame_step_ms=15, \n",
    "                                  sr=16000, energy_threshold=0.02, f0_threshold=300):\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    \n",
    "    class_map = {'adult voice': 'Adult Voice', 'baby cry': 'Baby Cry'}\n",
    "\n",
    "    # Tổng thời gian thực thi cho tất cả các tệp\n",
    "    total_time = 0\n",
    "    total_files = 0\n",
    "    file_times = []  # Dùng để lưu trữ thời gian xử lý từng tệp\n",
    "    \n",
    "    start_time = time.time()  # Bắt đầu tính thời gian toàn bộ quá trình\n",
    "\n",
    "    for class_name, expected_label in class_map.items():\n",
    "        class_folder = os.path.join(dataset_path, class_name)\n",
    "        if not os.path.isdir(class_folder):\n",
    "            print(f\"Directory not found: {class_folder}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Processing class: {class_name}\")\n",
    "        \n",
    "        for file_name in os.listdir(class_folder):\n",
    "            file_path = os.path.join(class_folder, file_name)\n",
    "            \n",
    "            # Bắt đầu đo thời gian cho từng tệp\n",
    "            file_start_time = time.time()\n",
    "\n",
    "            predicted_label = detect_audio_class(\n",
    "                file_path, \n",
    "                frame_length_ms=frame_length_ms, \n",
    "                frame_step_ms=frame_step_ms, \n",
    "                sr=sr, \n",
    "                energy_threshold=energy_threshold, \n",
    "                f0_threshold=f0_threshold\n",
    "            )\n",
    "            \n",
    "            # Ghi nhãn thực và nhãn dự đoán\n",
    "            true_labels.append(expected_label)\n",
    "            predicted_labels.append(predicted_label)\n",
    "\n",
    "            # Tính thời gian cho file hiện tại\n",
    "            file_end_time = time.time()\n",
    "            file_time = file_end_time - file_start_time\n",
    "            total_time += file_time\n",
    "            total_files += 1\n",
    "\n",
    "            # Lưu thời gian vào danh sách\n",
    "            file_times.append(file_time)\n",
    "\n",
    "    # Kết thúc đo thời gian\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    # Tính độ chính xác\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels) * 100\n",
    "    print(\"Độ chính xác: {:.2f}%\".format(accuracy))\n",
    "\n",
    "    # Tính và in độ chính xác theo từng lớp\n",
    "    print(\"\\nĐộ chính xác mỗi lớp:\")\n",
    "    for label in class_map.values():\n",
    "        class_true = [1 if l == label else 0 for l in true_labels]\n",
    "        class_pred = [1 if l == label else 0 for l in predicted_labels]\n",
    "        class_accuracy = accuracy_score(class_true, class_pred) * 100\n",
    "        print(f\"- {label}: {class_accuracy:.2f}%\")\n",
    "\n",
    "    # Báo cáo chi tiết\n",
    "    # print(\"\\nClassification Report:\")\n",
    "    print(classification_report(true_labels, predicted_labels, target_names=class_map.values(), zero_division=0))\n",
    "\n",
    "    # # In tổng thời gian cho tất cả quá trình đánh giá\n",
    "    print(f\"\\nTổng thời gian xử lý: {elapsed_time:.4f} seconds\")\n",
    "\n",
    "    # # Tính thời gian trung bình xử lý mỗi tệp\n",
    "    if total_files > 0:\n",
    "        avg_time_per_file = total_time / total_files\n",
    "        std_dev_time_per_file = np.std(file_times)  # Tính độ lệch chuẩn từ danh sách file_times\n",
    "        print(f\"Trung bình (Mean) của 1 tệp: {avg_time_per_file:.4f} seconds\")\n",
    "        print(f\"Độ lệch chuẩn (Std Dev) của 1 tệp: {std_dev_time_per_file:.4f} seconds\")\n",
    "        \n",
    "    # Ma trận nhầm lẫn\n",
    "    cm = confusion_matrix(true_labels, predicted_labels, labels=list(class_map.values()))\n",
    "    print(\"\\nMa trận nhầm lẫn:\")\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class: adult voice\n",
      "Frames classified as Silence: 114\n",
      "Frames classified as Voice: 28\n",
      "Frames classified as Cry: 29\n",
      "Frames classified as Silence: 98\n",
      "Frames classified as Voice: 32\n",
      "Frames classified as Cry: 41\n",
      "Frames classified as Silence: 140\n",
      "Frames classified as Voice: 77\n",
      "Frames classified as Cry: 29\n",
      "Frames classified as Silence: 138\n",
      "Frames classified as Voice: 35\n",
      "Frames classified as Cry: 56\n",
      "Frames classified as Silence: 148\n",
      "Frames classified as Voice: 64\n",
      "Frames classified as Cry: 34\n",
      "Frames classified as Silence: 132\n",
      "Frames classified as Voice: 35\n",
      "Frames classified as Cry: 62\n",
      "Frames classified as Silence: 134\n",
      "Frames classified as Voice: 52\n",
      "Frames classified as Cry: 60\n",
      "Frames classified as Silence: 142\n",
      "Frames classified as Voice: 25\n",
      "Frames classified as Cry: 79\n",
      "Frames classified as Silence: 128\n",
      "Frames classified as Voice: 45\n",
      "Frames classified as Cry: 73\n",
      "Frames classified as Silence: 149\n",
      "Frames classified as Voice: 53\n",
      "Frames classified as Cry: 44\n",
      "Frames classified as Silence: 147\n",
      "Frames classified as Voice: 64\n",
      "Frames classified as Cry: 35\n",
      "Frames classified as Silence: 161\n",
      "Frames classified as Voice: 19\n",
      "Frames classified as Cry: 66\n",
      "Frames classified as Silence: 152\n",
      "Frames classified as Voice: 11\n",
      "Frames classified as Cry: 83\n",
      "Frames classified as Silence: 138\n",
      "Frames classified as Voice: 60\n",
      "Frames classified as Cry: 48\n",
      "Frames classified as Silence: 129\n",
      "Frames classified as Voice: 82\n",
      "Frames classified as Cry: 35\n",
      "Frames classified as Silence: 129\n",
      "Frames classified as Voice: 76\n",
      "Frames classified as Cry: 41\n",
      "Frames classified as Silence: 121\n",
      "Frames classified as Voice: 68\n",
      "Frames classified as Cry: 57\n",
      "Frames classified as Silence: 173\n",
      "Frames classified as Voice: 51\n",
      "Frames classified as Cry: 22\n",
      "Frames classified as Silence: 163\n",
      "Frames classified as Voice: 76\n",
      "Frames classified as Cry: 7\n",
      "Frames classified as Silence: 163\n",
      "Frames classified as Voice: 43\n",
      "Frames classified as Cry: 40\n",
      "Frames classified as Silence: 168\n",
      "Frames classified as Voice: 43\n",
      "Frames classified as Cry: 35\n",
      "Frames classified as Silence: 127\n",
      "Frames classified as Voice: 92\n",
      "Frames classified as Cry: 27\n",
      "Frames classified as Silence: 136\n",
      "Frames classified as Voice: 79\n",
      "Frames classified as Cry: 31\n",
      "Frames classified as Silence: 168\n",
      "Frames classified as Voice: 48\n",
      "Frames classified as Cry: 30\n",
      "Frames classified as Silence: 143\n",
      "Frames classified as Voice: 69\n",
      "Frames classified as Cry: 34\n",
      "Frames classified as Silence: 133\n",
      "Frames classified as Voice: 68\n",
      "Frames classified as Cry: 45\n",
      "Frames classified as Silence: 185\n",
      "Frames classified as Voice: 51\n",
      "Frames classified as Cry: 10\n",
      "Frames classified as Silence: 147\n",
      "Frames classified as Voice: 90\n",
      "Frames classified as Cry: 9\n",
      "Frames classified as Silence: 161\n",
      "Frames classified as Voice: 71\n",
      "Frames classified as Cry: 14\n",
      "Frames classified as Silence: 145\n",
      "Frames classified as Voice: 86\n",
      "Frames classified as Cry: 15\n",
      "Frames classified as Silence: 143\n",
      "Frames classified as Voice: 71\n",
      "Frames classified as Cry: 32\n",
      "Frames classified as Silence: 140\n",
      "Frames classified as Voice: 59\n",
      "Frames classified as Cry: 47\n",
      "Frames classified as Silence: 172\n",
      "Frames classified as Voice: 57\n",
      "Frames classified as Cry: 17\n",
      "Frames classified as Silence: 116\n",
      "Frames classified as Voice: 91\n",
      "Frames classified as Cry: 39\n",
      "Frames classified as Silence: 131\n",
      "Frames classified as Voice: 87\n",
      "Frames classified as Cry: 28\n",
      "Frames classified as Silence: 162\n",
      "Frames classified as Voice: 57\n",
      "Frames classified as Cry: 27\n",
      "Frames classified as Silence: 140\n",
      "Frames classified as Voice: 52\n",
      "Frames classified as Cry: 54\n",
      "Frames classified as Silence: 132\n",
      "Frames classified as Voice: 60\n",
      "Frames classified as Cry: 54\n",
      "Frames classified as Silence: 133\n",
      "Frames classified as Voice: 90\n",
      "Frames classified as Cry: 23\n",
      "Frames classified as Silence: 111\n",
      "Frames classified as Voice: 118\n",
      "Frames classified as Cry: 17\n",
      "Frames classified as Silence: 136\n",
      "Frames classified as Voice: 57\n",
      "Frames classified as Cry: 53\n",
      "Frames classified as Silence: 174\n",
      "Frames classified as Voice: 36\n",
      "Frames classified as Cry: 17\n",
      "Frames classified as Silence: 154\n",
      "Frames classified as Voice: 55\n",
      "Frames classified as Cry: 18\n",
      "Frames classified as Silence: 79\n",
      "Frames classified as Voice: 57\n",
      "Frames classified as Cry: 107\n",
      "Frames classified as Silence: 73\n",
      "Frames classified as Voice: 75\n",
      "Frames classified as Cry: 95\n",
      "Frames classified as Silence: 124\n",
      "Frames classified as Voice: 39\n",
      "Frames classified as Cry: 64\n",
      "Frames classified as Silence: 101\n",
      "Frames classified as Voice: 42\n",
      "Frames classified as Cry: 100\n",
      "Frames classified as Silence: 106\n",
      "Frames classified as Voice: 18\n",
      "Frames classified as Cry: 74\n",
      "Frames classified as Silence: 123\n",
      "Frames classified as Voice: 46\n",
      "Frames classified as Cry: 29\n",
      "Frames classified as Silence: 116\n",
      "Frames classified as Voice: 32\n",
      "Frames classified as Cry: 50\n",
      "Frames classified as Silence: 123\n",
      "Frames classified as Voice: 33\n",
      "Frames classified as Cry: 42\n",
      "Frames classified as Silence: 113\n",
      "Frames classified as Voice: 30\n",
      "Frames classified as Cry: 55\n",
      "Frames classified as Silence: 113\n",
      "Frames classified as Voice: 42\n",
      "Frames classified as Cry: 43\n",
      "Frames classified as Silence: 123\n",
      "Frames classified as Voice: 22\n",
      "Frames classified as Cry: 53\n",
      "Frames classified as Silence: 100\n",
      "Frames classified as Voice: 56\n",
      "Frames classified as Cry: 42\n",
      "Frames classified as Silence: 109\n",
      "Frames classified as Voice: 58\n",
      "Frames classified as Cry: 31\n",
      "Frames classified as Silence: 150\n",
      "Frames classified as Voice: 21\n",
      "Frames classified as Cry: 27\n",
      "Frames classified as Silence: 88\n",
      "Frames classified as Voice: 69\n",
      "Frames classified as Cry: 41\n",
      "Frames classified as Silence: 145\n",
      "Frames classified as Voice: 22\n",
      "Frames classified as Cry: 31\n",
      "Frames classified as Silence: 109\n",
      "Frames classified as Voice: 61\n",
      "Frames classified as Cry: 28\n",
      "Frames classified as Silence: 136\n",
      "Frames classified as Voice: 20\n",
      "Frames classified as Cry: 42\n",
      "Frames classified as Silence: 145\n",
      "Frames classified as Voice: 10\n",
      "Frames classified as Cry: 43\n",
      "Frames classified as Silence: 155\n",
      "Frames classified as Voice: 16\n",
      "Frames classified as Cry: 27\n",
      "Frames classified as Silence: 124\n",
      "Frames classified as Voice: 16\n",
      "Frames classified as Cry: 58\n",
      "Frames classified as Silence: 110\n",
      "Frames classified as Voice: 37\n",
      "Frames classified as Cry: 51\n",
      "Frames classified as Silence: 144\n",
      "Frames classified as Voice: 38\n",
      "Frames classified as Cry: 61\n",
      "Frames classified as Silence: 109\n",
      "Frames classified as Voice: 95\n",
      "Frames classified as Cry: 23\n",
      "Frames classified as Silence: 111\n",
      "Frames classified as Voice: 88\n",
      "Frames classified as Cry: 28\n",
      "Frames classified as Silence: 166\n",
      "Frames classified as Voice: 28\n",
      "Frames classified as Cry: 33\n",
      "Frames classified as Silence: 147\n",
      "Frames classified as Voice: 26\n",
      "Frames classified as Cry: 54\n",
      "Frames classified as Silence: 147\n",
      "Frames classified as Voice: 13\n",
      "Frames classified as Cry: 67\n",
      "Frames classified as Silence: 175\n",
      "Frames classified as Voice: 8\n",
      "Frames classified as Cry: 44\n",
      "Frames classified as Silence: 191\n",
      "Frames classified as Voice: 7\n",
      "Frames classified as Cry: 45\n",
      "Frames classified as Silence: 150\n",
      "Frames classified as Voice: 38\n",
      "Frames classified as Cry: 55\n",
      "Frames classified as Silence: 187\n",
      "Frames classified as Voice: 1\n",
      "Frames classified as Cry: 39\n",
      "Frames classified as Silence: 184\n",
      "Frames classified as Voice: 5\n",
      "Frames classified as Cry: 38\n",
      "Frames classified as Silence: 174\n",
      "Frames classified as Voice: 16\n",
      "Frames classified as Cry: 53\n",
      "Frames classified as Silence: 159\n",
      "Frames classified as Voice: 16\n",
      "Frames classified as Cry: 68\n",
      "Frames classified as Silence: 139\n",
      "Frames classified as Voice: 30\n",
      "Frames classified as Cry: 58\n",
      "Frames classified as Silence: 149\n",
      "Frames classified as Voice: 23\n",
      "Frames classified as Cry: 55\n",
      "Frames classified as Silence: 183\n",
      "Frames classified as Voice: 12\n",
      "Frames classified as Cry: 48\n",
      "Frames classified as Silence: 197\n",
      "Frames classified as Voice: 1\n",
      "Frames classified as Cry: 45\n",
      "Frames classified as Silence: 171\n",
      "Frames classified as Voice: 30\n",
      "Frames classified as Cry: 26\n",
      "Frames classified as Silence: 162\n",
      "Frames classified as Voice: 79\n",
      "Frames classified as Cry: 2\n",
      "Frames classified as Silence: 173\n",
      "Frames classified as Voice: 30\n",
      "Frames classified as Cry: 40\n",
      "Frames classified as Silence: 164\n",
      "Frames classified as Voice: 36\n",
      "Frames classified as Cry: 43\n",
      "Frames classified as Silence: 129\n",
      "Frames classified as Voice: 53\n",
      "Frames classified as Cry: 45\n",
      "Frames classified as Silence: 133\n",
      "Frames classified as Voice: 29\n",
      "Frames classified as Cry: 65\n",
      "Frames classified as Silence: 183\n",
      "Frames classified as Voice: 24\n",
      "Frames classified as Cry: 20\n",
      "Frames classified as Silence: 129\n",
      "Frames classified as Voice: 64\n",
      "Frames classified as Cry: 34\n",
      "Frames classified as Silence: 140\n",
      "Frames classified as Voice: 55\n",
      "Frames classified as Cry: 32\n",
      "Frames classified as Silence: 132\n",
      "Frames classified as Voice: 23\n",
      "Frames classified as Cry: 72\n",
      "Frames classified as Silence: 124\n",
      "Frames classified as Voice: 50\n",
      "Frames classified as Cry: 53\n",
      "Frames classified as Silence: 76\n",
      "Frames classified as Voice: 77\n",
      "Frames classified as Cry: 74\n",
      "Frames classified as Silence: 128\n",
      "Frames classified as Voice: 45\n",
      "Frames classified as Cry: 54\n",
      "Frames classified as Silence: 132\n",
      "Frames classified as Voice: 62\n",
      "Frames classified as Cry: 33\n",
      "Frames classified as Silence: 130\n",
      "Frames classified as Voice: 11\n",
      "Frames classified as Cry: 62\n",
      "Frames classified as Silence: 127\n",
      "Frames classified as Voice: 159\n",
      "Frames classified as Cry: 4\n",
      "Frames classified as Silence: 128\n",
      "Frames classified as Voice: 42\n",
      "Frames classified as Cry: 33\n",
      "Frames classified as Silence: 176\n",
      "Frames classified as Voice: 12\n",
      "Frames classified as Cry: 39\n",
      "Processing class: baby cry\n",
      "Frames classified as Silence: 202\n",
      "Frames classified as Voice: 6\n",
      "Frames classified as Cry: 260\n",
      "Frames classified as Silence: 49\n",
      "Frames classified as Voice: 114\n",
      "Frames classified as Cry: 297\n",
      "Frames classified as Silence: 236\n",
      "Frames classified as Voice: 23\n",
      "Frames classified as Cry: 201\n",
      "Frames classified as Silence: 45\n",
      "Frames classified as Voice: 0\n",
      "Frames classified as Cry: 410\n",
      "Frames classified as Silence: 354\n",
      "Frames classified as Voice: 0\n",
      "Frames classified as Cry: 101\n",
      "Frames classified as Silence: 252\n",
      "Frames classified as Voice: 6\n",
      "Frames classified as Cry: 185\n",
      "Frames classified as Silence: 307\n",
      "Frames classified as Voice: 0\n",
      "Frames classified as Cry: 156\n",
      "Frames classified as Silence: 301\n",
      "Frames classified as Voice: 0\n",
      "Frames classified as Cry: 162\n",
      "Frames classified as Silence: 339\n",
      "Frames classified as Voice: 1\n",
      "Frames classified as Cry: 116\n",
      "Frames classified as Silence: 187\n",
      "Frames classified as Voice: 0\n",
      "Frames classified as Cry: 276\n",
      "Frames classified as Silence: 329\n",
      "Frames classified as Voice: 0\n",
      "Frames classified as Cry: 134\n",
      "Frames classified as Silence: 273\n",
      "Frames classified as Voice: 0\n",
      "Frames classified as Cry: 170\n",
      "Frames classified as Silence: 183\n",
      "Frames classified as Voice: 58\n",
      "Frames classified as Cry: 222\n",
      "Frames classified as Silence: 99\n",
      "Frames classified as Voice: 34\n",
      "Frames classified as Cry: 328\n",
      "Frames classified as Silence: 280\n",
      "Frames classified as Voice: 24\n",
      "Frames classified as Cry: 159\n",
      "Frames classified as Silence: 332\n",
      "Frames classified as Voice: 0\n",
      "Frames classified as Cry: 111\n",
      "Frames classified as Silence: 393\n",
      "Frames classified as Voice: 0\n",
      "Frames classified as Cry: 56\n",
      "Frames classified as Silence: 394\n",
      "Frames classified as Voice: 0\n",
      "Frames classified as Cry: 55\n",
      "Frames classified as Silence: 198\n",
      "Frames classified as Voice: 0\n",
      "Frames classified as Cry: 250\n",
      "Frames classified as Silence: 154\n",
      "Frames classified as Voice: 0\n",
      "Frames classified as Cry: 305\n",
      "Frames classified as Silence: 147\n",
      "Frames classified as Voice: 3\n",
      "Frames classified as Cry: 313\n",
      "Frames classified as Silence: 356\n",
      "Frames classified as Voice: 9\n",
      "Frames classified as Cry: 95\n",
      "Frames classified as Silence: 97\n",
      "Frames classified as Voice: 152\n",
      "Frames classified as Cry: 211\n",
      "Frames classified as Silence: 276\n",
      "Frames classified as Voice: 8\n",
      "Frames classified as Cry: 193\n",
      "Frames classified as Silence: 372\n",
      "Frames classified as Voice: 0\n",
      "Frames classified as Cry: 88\n",
      "Frames classified as Silence: 266\n",
      "Frames classified as Voice: 0\n",
      "Frames classified as Cry: 190\n",
      "Frames classified as Silence: 68\n",
      "Frames classified as Voice: 0\n",
      "Frames classified as Cry: 375\n",
      "Frames classified as Silence: 321\n",
      "Frames classified as Voice: 12\n",
      "Frames classified as Cry: 130\n",
      "Frames classified as Silence: 283\n",
      "Frames classified as Voice: 0\n",
      "Frames classified as Cry: 160\n",
      "Frames classified as Silence: 108\n",
      "Frames classified as Voice: 7\n",
      "Frames classified as Cry: 329\n",
      "Frames classified as Silence: 132\n",
      "Frames classified as Voice: 2\n",
      "Frames classified as Cry: 306\n",
      "Frames classified as Silence: 126\n",
      "Frames classified as Voice: 0\n",
      "Frames classified as Cry: 334\n",
      "Frames classified as Silence: 185\n",
      "Frames classified as Voice: 1\n",
      "Frames classified as Cry: 277\n",
      "Frames classified as Silence: 208\n",
      "Frames classified as Voice: 3\n",
      "Frames classified as Cry: 248\n",
      "Frames classified as Silence: 137\n",
      "Frames classified as Voice: 24\n",
      "Frames classified as Cry: 295\n",
      "Frames classified as Silence: 209\n",
      "Frames classified as Voice: 0\n",
      "Frames classified as Cry: 250\n",
      "Frames classified as Silence: 181\n",
      "Frames classified as Voice: 64\n",
      "Frames classified as Cry: 214\n",
      "Frames classified as Silence: 368\n",
      "Frames classified as Voice: 1\n",
      "Frames classified as Cry: 108\n",
      "Frames classified as Silence: 172\n",
      "Frames classified as Voice: 127\n",
      "Frames classified as Cry: 149\n",
      "Frames classified as Silence: 118\n",
      "Frames classified as Voice: 9\n",
      "Frames classified as Cry: 336\n",
      "Frames classified as Silence: 390\n",
      "Frames classified as Voice: 32\n",
      "Frames classified as Cry: 41\n",
      "Frames classified as Silence: 157\n",
      "Frames classified as Voice: 35\n",
      "Frames classified as Cry: 269\n",
      "Frames classified as Silence: 139\n",
      "Frames classified as Voice: 0\n",
      "Frames classified as Cry: 320\n",
      "Frames classified as Silence: 158\n",
      "Frames classified as Voice: 0\n",
      "Frames classified as Cry: 301\n",
      "Frames classified as Silence: 121\n",
      "Frames classified as Voice: 1\n",
      "Frames classified as Cry: 339\n",
      "Frames classified as Silence: 154\n",
      "Frames classified as Voice: 4\n",
      "Frames classified as Cry: 286\n",
      "Frames classified as Silence: 145\n",
      "Frames classified as Voice: 2\n",
      "Frames classified as Cry: 297\n",
      "Frames classified as Silence: 34\n",
      "Frames classified as Voice: 0\n",
      "Frames classified as Cry: 427\n",
      "Frames classified as Silence: 322\n",
      "Frames classified as Voice: 0\n",
      "Frames classified as Cry: 139\n",
      "Frames classified as Silence: 225\n",
      "Frames classified as Voice: 6\n",
      "Frames classified as Cry: 232\n",
      "Frames classified as Silence: 163\n",
      "Frames classified as Voice: 14\n",
      "Frames classified as Cry: 282\n",
      "Frames classified as Silence: 395\n",
      "Frames classified as Voice: 0\n",
      "Frames classified as Cry: 64\n",
      "Frames classified as Silence: 253\n",
      "Frames classified as Voice: 0\n",
      "Frames classified as Cry: 206\n",
      "Frames classified as Silence: 310\n",
      "Frames classified as Voice: 0\n",
      "Frames classified as Cry: 149\n",
      "Frames classified as Silence: 199\n",
      "Frames classified as Voice: 4\n",
      "Frames classified as Cry: 256\n",
      "Frames classified as Silence: 195\n",
      "Frames classified as Voice: 11\n",
      "Frames classified as Cry: 253\n",
      "Frames classified as Silence: 361\n",
      "Frames classified as Voice: 0\n",
      "Frames classified as Cry: 83\n",
      "Frames classified as Silence: 84\n",
      "Frames classified as Voice: 97\n",
      "Frames classified as Cry: 282\n",
      "Frames classified as Silence: 410\n",
      "Frames classified as Voice: 24\n",
      "Frames classified as Cry: 25\n",
      "Frames classified as Silence: 290\n",
      "Frames classified as Voice: 16\n",
      "Frames classified as Cry: 157\n",
      "Frames classified as Silence: 349\n",
      "Frames classified as Voice: 0\n",
      "Frames classified as Cry: 114\n",
      "Frames classified as Silence: 298\n",
      "Frames classified as Voice: 6\n",
      "Frames classified as Cry: 156\n",
      "Frames classified as Silence: 308\n",
      "Frames classified as Voice: 2\n",
      "Frames classified as Cry: 149\n",
      "Frames classified as Silence: 400\n",
      "Frames classified as Voice: 5\n",
      "Frames classified as Cry: 58\n",
      "Frames classified as Silence: 60\n",
      "Frames classified as Voice: 1\n",
      "Frames classified as Cry: 374\n",
      "Frames classified as Silence: 207\n",
      "Frames classified as Voice: 129\n",
      "Frames classified as Cry: 113\n",
      "Frames classified as Silence: 41\n",
      "Frames classified as Voice: 1\n",
      "Frames classified as Cry: 417\n",
      "Frames classified as Silence: 327\n",
      "Frames classified as Voice: 111\n",
      "Frames classified as Cry: 23\n",
      "Frames classified as Silence: 363\n",
      "Frames classified as Voice: 0\n",
      "Frames classified as Cry: 80\n",
      "Frames classified as Silence: 96\n",
      "Frames classified as Voice: 2\n",
      "Frames classified as Cry: 345\n",
      "Frames classified as Silence: 414\n",
      "Frames classified as Voice: 18\n",
      "Frames classified as Cry: 28\n",
      "Frames classified as Silence: 132\n",
      "Frames classified as Voice: 267\n",
      "Frames classified as Cry: 49\n",
      "Frames classified as Silence: 313\n",
      "Frames classified as Voice: 0\n",
      "Frames classified as Cry: 138\n",
      "Frames classified as Silence: 207\n",
      "Frames classified as Voice: 2\n",
      "Frames classified as Cry: 468\n",
      "Frames classified as Silence: 55\n",
      "Frames classified as Voice: 74\n",
      "Frames classified as Cry: 150\n",
      "Frames classified as Silence: 71\n",
      "Frames classified as Voice: 104\n",
      "Frames classified as Cry: 127\n",
      "Frames classified as Silence: 93\n",
      "Frames classified as Voice: 41\n",
      "Frames classified as Cry: 125\n",
      "Frames classified as Silence: 32\n",
      "Frames classified as Voice: 40\n",
      "Frames classified as Cry: 36\n",
      "Frames classified as Silence: 284\n",
      "Frames classified as Voice: 76\n",
      "Frames classified as Cry: 304\n",
      "Frames classified as Silence: 67\n",
      "Frames classified as Voice: 70\n",
      "Frames classified as Cry: 161\n",
      "Frames classified as Silence: 119\n",
      "Frames classified as Voice: 98\n",
      "Frames classified as Cry: 70\n",
      "Frames classified as Silence: 228\n",
      "Frames classified as Voice: 55\n",
      "Frames classified as Cry: 8\n",
      "Frames classified as Silence: 98\n",
      "Frames classified as Voice: 73\n",
      "Frames classified as Cry: 112\n",
      "Frames classified as Silence: 299\n",
      "Frames classified as Voice: 33\n",
      "Frames classified as Cry: 131\n",
      "Frames classified as Silence: 412\n",
      "Frames classified as Voice: 33\n",
      "Frames classified as Cry: 8\n",
      "Frames classified as Silence: 183\n",
      "Frames classified as Voice: 0\n",
      "Frames classified as Cry: 272\n",
      "Frames classified as Silence: 188\n",
      "Frames classified as Voice: 0\n",
      "Frames classified as Cry: 265\n",
      "Frames classified as Silence: 146\n",
      "Frames classified as Voice: 1\n",
      "Frames classified as Cry: 308\n",
      "Frames classified as Silence: 320\n",
      "Frames classified as Voice: 5\n",
      "Frames classified as Cry: 130\n",
      "Frames classified as Silence: 108\n",
      "Frames classified as Voice: 5\n",
      "Frames classified as Cry: 342\n",
      "Frames classified as Silence: 49\n",
      "Frames classified as Voice: 20\n",
      "Frames classified as Cry: 390\n",
      "Frames classified as Silence: 57\n",
      "Frames classified as Voice: 61\n",
      "Frames classified as Cry: 345\n",
      "Frames classified as Silence: 272\n",
      "Frames classified as Voice: 7\n",
      "Frames classified as Cry: 182\n",
      "Frames classified as Silence: 194\n",
      "Frames classified as Voice: 25\n",
      "Frames classified as Cry: 244\n",
      "Frames classified as Silence: 226\n",
      "Frames classified as Voice: 0\n",
      "Frames classified as Cry: 234\n",
      "Frames classified as Silence: 288\n",
      "Frames classified as Voice: 3\n",
      "Frames classified as Cry: 150\n",
      "Frames classified as Silence: 168\n",
      "Frames classified as Voice: 3\n",
      "Frames classified as Cry: 272\n",
      "Frames classified as Silence: 326\n",
      "Frames classified as Voice: 5\n",
      "Frames classified as Cry: 132\n",
      "Frames classified as Silence: 80\n",
      "Frames classified as Voice: 6\n",
      "Frames classified as Cry: 375\n",
      "Frames classified as Silence: 76\n",
      "Frames classified as Voice: 7\n",
      "Frames classified as Cry: 378\n",
      "Độ chính xác: 28.00%\n",
      "\n",
      "Độ chính xác mỗi lớp:\n",
      "- Adult Voice: 50.50%\n",
      "- Baby Cry: 75.50%\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 3, does not match size of target_names, 2. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m      2\u001b[0m     dataset_testing_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets_testing\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mevaluate_audio_classification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_testing_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 69\u001b[0m, in \u001b[0;36mevaluate_audio_classification\u001b[1;34m(dataset_path, frame_length_ms, frame_step_ms, sr, energy_threshold, f0_threshold)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Báo cáo chi tiết\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# print(\"\\nClassification Report:\")\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_map\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# # In tổng thời gian cho tất cả quá trình đánh giá\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTổng thời gian xử lý: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:2626\u001b[0m, in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   2620\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2621\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels size, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of target_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2622\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names)\n\u001b[0;32m   2623\u001b[0m             )\n\u001b[0;32m   2624\u001b[0m         )\n\u001b[0;32m   2625\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2626\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2627\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of classes, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2628\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m. Try specifying the labels \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2629\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names))\n\u001b[0;32m   2630\u001b[0m         )\n\u001b[0;32m   2631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2632\u001b[0m     target_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m l \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m labels]\n",
      "\u001b[1;31mValueError\u001b[0m: Number of classes, 3, does not match size of target_names, 2. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    dataset_testing_path = 'datasets_testing'\n",
    "    evaluate_audio_classification(dataset_testing_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
